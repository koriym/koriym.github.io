---
layout: post
title: "DeepSeek Moment"
date: 2025-02-06 22:58:00 +0900
comments: true
categories: ["blog"]
tags:
- AI
---

# DeepSeek Moment - Lex Fridman #459

<iframe width="560" height="315" src="https://www.youtube.com/embed/_1f-o0nqpEI?si=EEDkYxiOdlCWEdEy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Lex Fridman氏がホストするポッドキャスト、[DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters - LexFridman Podcast #459](https://youtu.be/_1f-o0nqpEI?si=_N93BXUfVMfaXmKC) における対談は約5時間超(!)にわたる大作で、AIの最新動向について類を見ない圧倒的なスケールです。咀嚼するため、聞き取りと翻訳を行い記事にまとめました。

ポッドキャストでは何度も今この時を「DeepSeek Moment」と称して、世界のAI業界が劇的変化を遂げている転換点を描き出す、非常に濃厚な内容でした。DeepSeekの革新、メガクラスターの台頭、米中AI競争、電力インフラの限界や半導体覇権の綱引き...。技術と地政学が交差する歴史的瞬間を捉えた対談です。

## 目次

1. [イントロダクション — 深まるAI競争と「DeepSeek Moment」](#introduction)  
2. [DeepSeekの技術革新 — 制約から生まれたブレークスルー](#deepseek)  
3. [インフラ革命 — 電力がAIの新たなボトルネックに](#infrastructure)  
4. [地政学との交差 — 技術覇権競争の新局面](#geopolitics)  
5. [メガクラスターの時代 — 人類史上最大の計算機構築競争](#megacluster)  
6. [トレーニングの革新 — 「Chain of Thought」の衝撃](#chain-of-thought)  
7. [インフラのボトルネック — 推論コストの課題](#bottleneck)  
8. [オープンソースの未来 — AIモデルはどこまで公開できるのか](#opensource)  
9. [未来への示唆 — スピードとリスクをどう捉えるか](#future)

---

<a name="introduction"></a>
## イントロダクション — 深まるAI競争と「DeepSeek Moment」

まず、このエピソードがどんな背景を持つのか、登場人物を見ていきましょう。

### ポッドキャストの趣旨とゲスト
<img src="https://pbs.twimg.com/profile_images/1854713863817646088/nTmsz7jR_400x400.jpg" width=100px alt="Lex Fridman" style="float: left; margin-right: 20px;">
- **Lex Fridman**: MITの研究者であり、「AI」「ロボティクス」「宇宙」「自動運転」など、多彩な分野の最先端ゲストと対談を行うポッドキャストを運営。技術とともに人文的・哲学的視点を交えて深く掘り下げるスタイルが特徴です。

<img src="https://pbs.twimg.com/profile_images/1284174246726586370/0v85zsJR_400x400.jpg" width=100px alt="Dylan Patel" style="float: left; margin-right: 20px;">
- **Dylan Patel**: 半導体・ハードウェア分野の分析メディア「[Semi Analysis](https://semianalysis.com/)」を主宰し、NVIDIAやTSMC、Intelなどシリコン業界の動向を鋭くウォッチしています。AIのインフラ面にも精通しており、特にGPU供給やデータセンター構築戦略の最前線情報を熟知しているのが強み。この人が持つ情報の量と質は圧巻です！

<img src="https://pbs.twimg.com/profile_images/1732079679610425344/YqSwiBqA_400x400.jpg" width=100px alt="Nathan Lambert" style="float: left; margin-right: 20px;">
- **Nathan Lambert**: Allen Institute for AI（通称AI2）の研究者であり、大規模言語モデル（LLM）の学習手法やポストトレーニング（特に強化学習＋検証可能タスク）自身でブログを運営し、業界動向をまとめて提言なども行ってます。

### このエピソードの背景

私自身はAIの専門知識を持ち合わせていませんが、一ユーザーとしてDeepSeekに以前から注目していました。昨年7月にAPIの利用を開始したのですが、その頃の料金はChatGPTと比べて98%~99%オフの料金で「なぜこんなに安価なのか？」「なぜ中国の大手ではなく無名のスタートアップなのか」と不思議に思ったのがきっかけです。DeepSeekの技術や組織を知るるうちに、中国国内でも特異な存在だと分かったり、AIの専門家の人たちが称賛してるのも知り、同時に「どうしてこんなにすごい組織があまり注目されていないのか？」という疑問を持つようになりました。

そこでLex FridmanがDeepSeekを取り上げる予告をしていたときから、このエピソードをとても楽しみにしていましたが、実際の内容は予想をはるかに超えるものでした。

### このポッドキャストの価値

AIや半導体、米中問題などを扱う番組は多いものの、「最先端技術と地政学・インフラ構築」を一度に横断して論じられる人材は貴重です。Dylan氏とNathan氏は、「物理と電力」のハードウェア観点、「学習手法とモデルアーキテクチャ」のソフトウェア観点を深く理解しており、さらに世界のAI企業がしのぎを削る現場情報まで幅広くカバーできるという、稀有な組み合わせです。

R1がリリースされる前、Xで「DeepSeek」を検索すると、数は少ないながらも識者による濃い考察が見られました。ところがR1のリリース後はゴシップめいたノイズ投稿が急増。しかし中ではNathan Lambertさんの投稿がいちばんフェアで興味深いと感じました。その彼がこのポッドキャストに出演すると分かり、見逃すわけにはいかないと思ったのです。

今回の対談によって、「いま本当に起きている最前線」を一度に俯瞰できた点に意義がある思います。その中心に「DeepSeek」という中国発の企業が登場し、大きな転換点を生み出している。それは単に中国の会社が「うまくやっている」というだけではなく、真の技術的革新と地政学的状況が複雑に絡み合っているところが非常に面白いのだと感じました。

---

<a name="deepseek"></a>
## 2. DeepSeekの技術革新 — 制約から生まれたブレークスルー

鍵となるのは、中国系AIスタートアップの**DeepSeek（深度求索）**による躍進です。もともとクオンツ系ヘッジファンド[High Flyer](https://www.high-flyer.cn/en/#index)を母体に、GPUを大量に保有していたとされ、高度な自然言語処理モデル「DeepSeek-V3」と、リースニング特化モデル「DeepSeek-R1」をリリースしました。

### H800の制限を逆手に取った最適化

> (00:32:18) DeepSeekは、利用できるGPUに制限があり、さらに中国に合法的に輸入されたGPU（密輸品ではなく正規品）を使ってモデルをトレーニングする必要があったため、通信の最適化に取り組む必要がありました。その解決策の一つが、NVIDIAのNCCLライブラリを単に呼び出すのではなく、自分たちで通信をスケジューリングするという方法でした。この手法は、一部の研究機関でも採用されています。

通常、AIの最先端モデルの学習・推論にはNVIDIA H100やA100といった高性能GPUが好まれますが、中国向けにはFLOPsや通信帯域が制限された“H800”しか正規入手できません。ところがDeepSeekはそれを逆手に取り、

1. CUDAよりも下のPTXレベルで通信を制御
2. NCCL（NVIDIA標準ライブラリ）を使わず独自通信アルゴリズムを実装
3. GPUのSM(Streaming Multiprocessor)リソースを通信と演算に動的に割り当て

といった大胆なアプローチで大規模学習を効率化しました。従来「帯域不足で大規模学習は困難」と見られていたのを、ソフトウェアによる超低レイヤー最適化でカバーしたのが最大の特徴と言われています。

#### SM単位での手動スケジューリング

> 中国国内に正規輸入されるH800（H100の中国向け制限版）は通信帯域が制限されているので、効率を上げようとしたらNCCLの既存機能だけでは限界がある。そこでDeepSeekはSM（Streaming Multiprocessor、GPU上の演算ユニット）のどれを通信に使い、どれを演算に使うかをPTXレベルで細かく指定しているようなんです。こういった実装はかなり異例ですが、ものすごい効率化が可能になる。

NVIDIAが提供するNCCLは、GPU間通信を自動化してくれる便利なライブラリですが、H800のような制限版GPUには最適化が甘いです。そこでDeepSeekはGPUのStreaming Multiprocessor（SM）を細かく制御し、「いつ演算し、いつ通信をするか」をスケジューリングすることでアイドル時間を最小化。帯域がわずかでもフルに使えるようにし、大規模モデルの効率を飛躍的に上げたそうです。

### MoEの大胆なスパース化 — 6000億が実質370億?

> (00:35:00)DeepSeekの実装は非常に複雑です。特にMixture of Experts（MoE）の採用が大きな特徴です。これまでMoEを導入したモデルは存在しましたが、一般的には8個または16個のエキスパートを持ち、そのうち2つを活性化する程度でした。この活性化割合のことを「スパース係数（sparsity factor）」や「使用率（usage）」と呼びます。

例えば、MistralのMixtralモデルは、1/4のエキスパートを活性化する設計でした。このモデルが評価された理由の一つは、このアプローチにより効率性と性能を両立した点にあります。OpenAIをはじめとする大手の研究機関もMoEを採用しています。

しかし、DeepSeekのモデルでは、スパース係数がさらに高くなっています。従来の「8個のうち2個を活性化」するようなモデルとは異なり、「256個のうち8個を活性化（！）」する形を取っています。これは、最先端の研究機関でも最近になってようやく試み始めたような手法です。

さらにDeepSeekは**Mixture of Experts（MoE）**を極端にスパース化し、「実際に使う計算量」を抑える手法を導入しました。表向き6000億パラメータという巨大モデルでも、推論ごとに370億パラメータ程度しか呼び出さないため、コストが大幅に削減されるそうです。

### R1の「推論可視化」とチェインオブソート公開

> (00:21:06) 最終的にはR1がトーンを変えて、「結論」パートに入ります。つまり、先に展開した思考プロセスを要約し、最終回答を述べる。DeepSeekの注目が集まったのは、この「モデルが思考過程を明示してくれる」点が大きいと思います。具体的には、「理由を述べるパート」と「回答を述べるパート」を2段階に分けるよう訓練されていて、後者に入るときに特殊なトークンを挟んで切り替える仕組みがある。OpenAIだと、ユーザーインターフェイスで推論の中間ステップをまとめて表示し、「まず問題を分解して計算して…」という流れをダイアログ風に見せてくれたりします。DeepSeekの場合はそのまま膨大なトークンを出しながら進むんですね。

もう一つの特徴が、**DeepSeek-R1**における「思考過程の可視化」です。ユーザーがチャットすると、膨大な自問自答テキスト、思考過程（チェインオブソート）が表示され、最後に結論を提示します。ChatGPTなどがチェインオブソートを安全性のため隠すのとは対照的で、研究者や一部ユーザーからは「モデルの誤りを見つけやすい」と好評です。一方で、中間出力から不適切情報が漏れる恐れなど課題も指摘されています。

---

<a name="infrastructure"></a>
## インフラ革命 — 電力がAIの新たなボトルネックに

> (03:37:45) 一般的に、従来のデータセンターのタスクは分散システムの問題でした。例えば、Googleで検索リクエストを送信すると、最寄りのデータセンターにルーティングされ、検索ランキングが処理され、結果が返されるという仕組みです。しかし、AIの登場により、この構造が大きく変わりました。

DeepSeekの話は「制約を逆手に取ったブレークスルー」として非常に際立ちますが、番組全体では「電力・冷却インフラがAI拡張を阻むボトルネック」というテーマも大きく語られました。

### 電力コストのインパクトが増大

現在、GPUを何万台も集めるメガクラスターの建設が相次いでおり、電力需要がこれまでにないペースで急増中です。OpenAIとMicrosoftの**Stargate**は、2.2ギガワット（GW）という膨大な電力を消費する見込みで、これは中規模都市全体を上回るレベルと言われ、調べると佐賀県の玄海原子力発電所の出力に匹敵するほどの電力のようです。

さらに複数のAI企業が同様の規模でデータセンターを建設すれば、その電力はどこから持ってくるのか？ 送電網の整備は？ 環境負荷は？ など、従来のクラウド時代には想定されなかったスケールの問題が噴出します。

> MetaがPyTorchに「PyTorch.powerplant_no_blow_up」というフラグを追加し、アイドル時に偽の演算をすることで電力スパイクを回避する

というエピソードも紹介されていました。
つまり、もし計算が止まって電力消費が急激に低下したときに、発電所そのものにダメージを与えないようにするために『偽の計算』を行い無駄な電力消費をして"powerplant_no_blow_up"（発電所が吹き飛ばない）を実現するというものです。信じられないようなフラグ名です。

### 各社の電力戦略の違い

> (03:43:34) OpenAIが発表した[Stargateプロジェクト](https://group.softbank/news/press/20250122) は、アリゾナ州とテキサス州アビリーンに設置される予定ですが、フルスケールの完成時には2.2ギガワット（2200メガワット）の電力が必要になります。この規模は、一般的な都市全体の電力消費量を超えるものです。これは、プレトレーニングやポストトレーニングを含めたAIモデルの学習を行うためのインフラです。 AIが進化するにつれて、データセンターの規模も前例のないレベルに達しており、今後さらに大規模なクラスターが登場することが予想されます。
>(03:44:22) これはもう狂気の沙汰ですね。

それぞれの戦略の違いが興味深いです。

- **OpenAI / Microsoft**: 連邦政府の協力（あるいは規制緩和）も得て、大規模発電所の近隣にメガデータセンターを建てる戦略。これにより電力のロスを最小限にできる。既存のエネルギー企業と連携しながら急ピッチで建設中。

- **Meta**: 既存のインフラを拡張しつつ、天然ガスや再生可能エネルギーなどを段階的に導入する。レコメンドや広告システムにAIを使うため、推論需要に応じてインフラ規模を増やす。Googleとも似たような分散戦略をとり、大量の小～中規模データセンターを地域分散。

- **xAI**: Elon Muskの独断的な「突き抜けた行動力」で、メンフィスの古い工場を買収・改装し、独自の液冷システムを導入。発電設備も自前で整備して、電力供給の柔軟性を極限まで高めている。時間短縮と高密度化の両立を目指す。Elonらしい大胆な戦略。

それぞれのキャラクターが浮かび上がるような戦略の違いが面白いですね。

### 冷却技術の進化

GPUクラスタはCPUクラスタより発熱量が大きく、空冷では限界に達しやすいという問題があります。そこで**液冷**を取り入れる事例が増え、xAIのように全面液冷に挑む企業も出てきました。初期コストや運用リスクは高い一方、将来的には更なる高密度化や効率化が可能になる期待があります。

---

<a name="geopolitics"></a>
## 地政学との交差 — 技術覇権競争の新局面

### 米中のAI競争と輸出規制

> (01:00:52)ちょっと話を広げますが、輸出規制（エクスポートコントロール）の動機や考え方はどんなものでしょう？ダリオ・アモデイが最近ブログで書いていましたよね。「もしAIが極めて強力になり、彼によれば2026年までにAGIあるいは超強力なAIが誕生するとしたら、それを持った国は軍事的に圧倒的なアドバンテージを得るだろう」と。
>(01:01:19)で、合衆国は民主国家だけど、中国は権威主義的（あるいはその要素がある）なので、「超強力AIを民主国家側が独占することが望ましい、二極化して両大国が強力AIを持つと世界はもっと複雑になる」という理屈です。なので、米国はエクスポートコントロールを使って中国が大規模な学習（AGIを作るのに要るであろう大規模学習）をできないように時間を稼ぎたい、と。これが彼の主張なんですね。

これは話題になった[Anthropic CEOのブログ](https://darioamodei.com/on-deepseek-and-export-controls)のことです。[記事:DeepSeek登場後も、AIチップの輸出規制は必要──Anthropicダリオ・アモデイ氏](https://ascii.jp/elem/000/004/248/4248458/#eid4155923)

米国によるGPU輸出規制、そしてそれを逆手に取り最適化したDeepSeekとの対比は、番組の重要トピックです。FLOPsや帯域が制限されたH800でさえ、本気で最適化すれば最高水準モデルを作れることが示され、米中の“いたちごっこ”感が強まっています。

さらに密輸・第三国調達・自社ASICなどの選択肢があり、厳しいエクスポートコントロールをしても中国が完全に停滞するわけではないのでは、という見方も出ていました。

### 中国政府の1兆人民元投資とその意図

> (01:24:43)
アメリカでは、AIに関する議論が活発に行われています。トランプがDeepSeekやStargateについて話題にしたのと同じ週に、バイデン政権もAIに関する議論を重ねました。つまり、アメリカの政府はこの問題を真剣に考えています。
>一方、中国では、先週になってようやくDeepSeekの代表が中国政府のナンバー2と会談しました。習近平との会談はまだ実現しておらず、中国政府がAIに本格的な資金を投入し始めたのはつい最近のことです。

中国政府は、約1兆人民元（約1600億ドル）の補助金をAI産業向けに発表しましたが、これはMicrosoft、Meta、Googleが今年AIインフラに投じる金額とほぼ同じ規模です。つまり、中国は今ようやくこの競争に本格参入しようとしている段階です。

DeepSeekの成功を背景に、中国政府が1600億ドル規模の投資を公言し、HuaweiやAlibaba、Baiduなどへ波及していく可能性があります。同時に、TSMCや台湾問題がリスクとして浮上し、米国はCHIPS法で生産を国内誘致しようとするが実現には時間がかかるという展開です。

### ソフトパワーとしてのAI

AIは軍事・経済だけでなく、情報操作や世論形成にも強力なツールとなり得ます。巨大言語モデルをSNSやコミュニケーション基盤に組み込めば、政治的利用も容易であり、米中の攻防はその面でも激化しそうだと番組では語られていました。

---

<a name="megacluster"></a>
## メガクラスターの時代 — 人類史上最大の計算機構築競争

> (03:43:34) OpenAIが発表したStargateは、アリゾナ州とテキサス州アビリーンに設置される予定ですが、フルスケールの完成時には2.2ギガワット（2200メガワット）の電力が必要になります。 この規模は、一般的な都市全体の電力消費量を超えるものです。...これはもう狂気の沙汰ですね。結局、各データセンターの隣に原子力発電所を建設することになるのでしょうか？

### クラスタ規模が桁違いに

2020年前後、GPT-3が数千台～1万台のGPUを使って数週間学習したというだけでも大ニュースだったのが、今は**10万枚～20万枚規模のGPUクラスター**が複数企業によって同時多発的に構築されています。番組では、Elon MuskのxAIがメンフィスで20万台まとめ、OpenAIがStargateで10万台規模（将来的にはさらに倍かそれ以上）を見込むといった数字がポンポン飛び出していました。  

こうなるともはや「データセンター」というより「小さな発電所の横に巨大工業プラントを建てる」イメージに近く、エネルギー・冷却・ネットワークすべてが前例のないスケールに到達します。人類史における最大級のコンピュータ構築レースと言えるでしょう。

この背景に各社が「AGIの実現が数年以内」と見ていて、それを実現するためのあらゆる施作を急いでいるという点がありそうです。

### 集約型vs分散型

> (03:56:31) メガクラスターは推論（インフェレンス）には適していません。推論のワークロードは、30メガワット、50メガワット、100メガワット規模の小規模データセンターに分散させるのが一般的です。一方で、トレーニングには、超高帯域で接続された大規模なデータセンターが必要になります。そのため、トレーニングはメガクラスターで行い、推論は分散型データセンターで処理するのが理想的です。

そのクラスターの構築方法にも違いがあります。

- **集約型（例: xAI, 一部のStargateプラン）**
    - 全GPUを1ヶ所に集めることで、モデル学習時の通信レイテンシを最小化し、大規模並列学習を効率化。
    - ただし一度に数ギガワット単位の電力が必要となり、土地や送電網の許認可が大変。故障リスクや自然災害リスクも高い。
- **分散型（例: Meta, Google）**
    - 全世界や国内各地にデータセンターを置き、ユーザーに近い場所で推論を行う仕組み。
    - スケールは大きいが、学習時には複数拠点をネットワークで繋ぐ必要があり、ブロックしない通信と同期が課題に。
    - 災害リスクや政治リスクは分散できるが、アーキテクチャ設計が一層複雑になる。

### ソフトウェアスタックと低レイヤー最適化

DeepSeekが示したように、通信ライブラリやスケジューリングを徹底的に最適化しないと莫大なクラスターを持て余す可能性があります。MetaがPyTorchに発電所対策フラグを入れたように、前例のないレベルでソフトウェアが物理インフラと直接関わる時代になっているというわけです。

---

<a name="chain-of-thought"></a>
## トレーニングの革新 — 思考の連鎖の衝撃

### DeepSeek-R1の新しい対話体験

> (00:21:06) この「モデルが思考過程を明示してくれる」点が大きいと思います。具体的には、「理由を述べるパート」と「回答を述べるパート」を2段階に分けるよう訓練されていて、後者に入るときに特殊なトークンを挟んで切り替える仕組みがある。OpenAIだと、ユーザーインターフェイスで推論の中間ステップをまとめて表示し、「まず問題を分解して計算して…」という流れをダイアログ風に見せてくれたりします。DeepSeekの場合はそのまま膨大なトークンを出しながら進むんですね。

DeepSeek-R1は推論ステップをすべて公開する「思考過程の可視化」を採用し、ユーザーがモデルの脳内を眺めるような体験を可能にしています。誤りの追跡や研究用途には利点があるものの、チェインオブソートに不適切情報が混じるリスクもあり、一長一短です。

### 強化学習×検証可能タスクの融合

R1など「数式やコードなど検証可能な回答」を対象とするモデルでは、チェインオブソートの中間ステップを強化学習で最適化しやすいと言われます。Andrej Karpathy氏が指摘する「模倣学習から試行錯誤学習（RL）へ」シフトする潮流の最前線を象徴する取り組みとして注目されています。


> 私が好きな単純な例はAlphaGoで、学習の方法は２つあり、1）熟練棋士の真似をして学習している、2）ゲームに勝つために強化学習する。（勝ち負けを報酬として繰り返し自分同士で対戦をする）
> ディープラーニングのほとんどすべての衝撃的な結果、そしてすべての*マジック*の源は常に2である。2こそがあなたを驚かせる。
> — Andrej Karpathy氏

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I don&#39;t have too too much to add on top of this earlier post on V3 and I think it applies to R1 too (which is the more recent, thinking equivalent).<br><br>I will say that Deep Learning has a legendary ravenous appetite for compute, like no other algorithm that has ever been developed… <a href="https://t.co/mX5kiQEJPX">https://t.co/mX5kiQEJPX</a></p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1883941452738355376?ref_src=twsrc%5Etfw">January 27, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

この投稿は多くの反響を呼んでいたのでよく覚えてましたがポッドキャストでもとりあげられました。o1やR1など推論モデルが注目される証です。またいくつかの発言がこれが**1**(o1, R1)に過ぎないこと、つまりまだはじまり(1)に過ぎないすぎず、飛躍的な発展の可能性があることを示唆していました。

### 倫理的・社会的インパクト

思考過程をすべて公開すると、中間ステップがユーザーに与える影響や、誤った差別表現・秘密情報が含まれる可能性が高まり、現時点では欧米の大手モデルが慎重になっている要因でもあります。一方で、DeepSeekのようにリースニングを最大化するアプローチには大きな研究的価値があり、安全性との折り合いが問題になってくるでしょう。

---

<a name="bottleneck"></a>
## インフラのボトルネック — 推論コストの課題

### トレーニングと推論のコスト構造

> (01:14:35)
例を挙げると、ChatGPTが検索を一気に変えると思われたけど、実際にはコストが高すぎてGoogle検索の全部を置き換えるには物理的に無理だった。今後、AGI的な性能を出すタスクになるほど1回の推論コストが何ドル〜何十ドルにもなる、といったことが起きるわけで。その時、「誰がそれを大規模に運用して、どの課題に適用するか」は大問題です。Darioの懸念は、中国が軍事分野にすぐ適用するのではないか、ということでしょう。

従来は「巨大モデルを学習するコスト」が話題の中心でしたが、運用フェーズに入ると**推論コスト**が深刻化します。DeepSeekもR1運用で新規ユーザーを停止する事態に陥ったように、実際のユーザー数をさばけるだけのGPUリソースを恒常的に用意し続けるのは容易ではありません。

### 7.2 リースニング（推論）モデルと高額APIの出現

OpenAIのo1・o3-mini、DeepSeek-R1などリーゾニング特化型モデルは、一つのクエリに多数の内部試行を行うため、GPU負荷が高くなりがちです。これがAPI利用料金を押し上げ、最終的に一般ユーザーが気軽に使えるものではなくなる恐れがあると指摘されています。

### 7.3 今後の解決策や見通し

- **専用ハードウェア**: GPUアーキテクチャの進化や、Googleの次世代TPU、Meta/Amazonの自社チップなど  
- **モデル圧縮・蒸留**: 大きなモデルを小型化し、推論を軽量化  
- **分散推論**: 地理的に分散したサーバーで低レイテンシを狙う

ただし、いずれも相応の課題を含んでおり、長期的に見れば大幅な革新が必要だと考えられています。

DeepSeekも部分的にOpenAIモデルを蒸留したかもしれない、という疑惑が報道されました。しかし番組の中ではOpenAPIが出力したものとその生成でつくられたコンテンツに区別がつかないし、そもそも違反したとしてもアカウント停止ぐらいの処置しか取れようが無い。なんでそんな事を話題にするんだという取り上げ方でした。 ３人の口ぶりを聞いているとこの件の続報は無いように思いましたがどうでしょうか。

---

<a name="opensource"></a>
## オープンソースの未来 — AIモデルはどこまで公開できるのか

### DeepSeekのMITライセンスインパクト

DeepSeekがクローズドデータながらR1モデルをオープンウェイト（MITライセンスに近い形）で公開したことは、業界に大きな衝撃を与えました。MetaのLlamaは制限付きライセンスで、本当の意味でオープンソースとは言いづらかったのに対し、DeepSeekは商用利用も含めほぼ制限がない形です。
しかし、学習再現には数万〜数十万台のGPUが要るかもしれないため、誰もが気軽に改良できるわけでもありません。

### オープンソースAIの本質的障壁

> (00:06:17)
オープンソースAIの定義や本質は、まだ議論の真っ最中です。オープンソースソフトウェアの世界では、改変の自由や自分で引き継ぐ自由など、多くの自由を認める歴史がありますが、それをAIに当てはめるとどうなるかは、まだ固まっていない面があるんですね。

>オープンソースの世界を「完全にコードもデータもウェイトも公開」が極端な一端とすると、DeepSeekはそれほどオープンではないにせよ、少なくとも「オープンウェイト」でMITライセンスに近い形。つまりオープンソースムーブメント的には「善玉」に近い感じなんでしょう？

>ええ。DeepSeekはAIの知見を広めるために素晴らしい貢献をしていると思います。彼らの論文は非常に詳細で、世界の他の研究チームがトレーニング手法を改善するのに役立つ情報が多い。ライセンス面で言うと、DeepSeek-R1モデルは非常に寛容なライセンス、つまりMITライセンスです。これは商用利用において制限がなく、ユースケース制限もなく、生成された出力を使って合成データを作るのも自由ということです。

ソフトウェアの場合、コードを公開すれば誰もがビルドして利用可能です。しかし、AIモデルでは

1. 学習データの著作権・プライバシー問題  
2. 学習にかかる莫大なコスト  
3. 推論運用に必要な大規模インフラ  

などの障壁があります。ソフトウェアのように「コードを公開すれば誰でも同じものがビルドできる」という単純な構図ではありません。それでも蒸留・微調整などのコミュニティ活動は活発で、「オープンソースの理念が完全に消えたわけではない」とは言えそうです。

### 別種のエコシステム形成

> (04:45:18) ただし、オープンソースAIは、オープンソースソフトウェアとは異なる性質** を持っています。例えば、ソフトウェアは一度作れば再利用できます。しかし、AIモデルの場合、データとトレーニングコードを公開しても、それを改良するには膨大な計算資源と専門知識が必要** です。そのため、オープンソースAIには、オープンソースソフトウェアほどのフィードバックループが存在しない** のが現状です。

完全オープンにこだわらない「準オープン」「コラボレーティブ・ライセンス」といった形も考えられます。DeepSeekが今後どう動くのか、OpenAIやGoogleがどの程度オープン化を進めるのかは、依然不透明です。
先日、サムアルトマンが東京でオープンソースに言及しましたがDeepSeekが影響を与えたことは確実です。

---

<a name="future"></a>
## 未来への示唆 — スピードとリスクをどう捉えるか

### かつてない革新ペース

> (04:21:07)
つまり、彼らの目標はAGIですね？
> その通りです。エージェント（AI Agents）やAGIを開発できれば、莫大な利益を生み出せるか、少なくともすべてのコストを回収できると考えています。
> これは、輸出規制（export control）とも関係しています。もしAGIが5〜10年後に実現すると考えているなら、長期的な投資戦略を取ることができます。しかし、多くのAIラボは「AGIは2〜3年以内に来る」と予想しています。
> この前提があるため、彼らの行動は「AGIが2年後に来る場合」と「AGIが10年後に来る場合」では全く異なるものになります。もしAGIがすぐに実現すると信じているなら、すべてを投じて開発を加速することが合理的な選択肢になるでしょう。

まとめると、このポッドキャストは「AI産業がどれだけ急速に加速しているか」を鮮烈な事例で見せつけています。
GPT-3からわずか数年で、電力消費がギガワット級、GPUクラスターが10万台規模という世界になり、あと3〜5年でAGIが一部到来するという見立ても多数出ています。

### リスク管理と国際協調

このスピード感でインフラや技術が巨大化すれば、安全保障や社会操作、環境負荷など未曾有のリスクが高まります。ローカルな法整備だけでは追いつかず、国際協調やグローバルルールの必要性を指摘する声が大きくなっているのは自然な流れでしょう。

### 人類の知的能力を広げるか、それとも破壊するか

対談の中では、AIが人間の知的作業を大幅に拡張する素晴らしい側面と、危険な側面がともに語られていました。DeepSeek-R1のような思考過程可視化は革新的なメリットがある一方、悪用されれば社会操作や誤情報拡散がさらに巧妙化するリスクがあるわけです。善と悪の両面をどう制御するかが今後の最大課題と言えます。

---

## 終わりに

今回のエピソードに登場した「DeepSeek Moment」は、AI業界がまさに歴史的転換点を迎えていることを象徴しています。

1. 中国企業が輸出規制下でH800の帯域制限を逆手に技術革新  
2. アメリカ企業がStargateなどでギガワット級のメガデータセンターを爆速で建設  
3. オープンソースかクローズドか、巨大モデルをどう扱うかの議論が熱を帯びる  
4. 米中間の輸出規制や地政学的緊張がテック業界をさらに加速  
5. リースニングモデルの推論コスト高騰や、チェインオブソート可視化がもたらす新課題  

これらが一挙に押し寄せる中、Lex Fridman氏のポッドキャスト#459は、専門家2名の知見を通して現在のカオス的な状況を克明に描いており、AIの未来に興味を持つ人には必聴の内容でだと思いました。実際、このポッドキャストは評判をよびまた別の議論をよんでいるようです。

<iframe width="560" height="315" src="https://www.youtube.com/embed/j8chNXhA8o8?si=vLKf1WO-ELpWgDVv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

いずれにせよどんなシナリオであれ、AIは今後もスケールアップを続け、社会と経済を大きく揺さぶる存在となるでしょう。そのスピードは驚異的で、かつての産業革命と比べてもはるかに急激です。 この激流の中で、企業・技術者・政府・市民社会がどのような選択をしていくのか。突如現れた感のあるDeepSeek R1のリリースは、すでに顕在化していた技術と地政学の交差点に、新たな光を当てる出来事となりました。

ポッドキャストでも「DeepSeek Moment」は今後語り継がれるだろうと示唆されていましたが、私自身もこのポッドキャストを聞いてそう感じました。最後に、とくに印象深かったXの投稿を引用します。

> **DeepSeek v3とR1のオープンソースリリースは、1994年にリリースされたMosaicのNetscapeブラウザと似た雰囲気を持っている。**
> **それはまったく新しい時代の到来を告げるものであり、インターネットの発展における極めて重要な瞬間であった。**
> **それが中国企業であることが、筋書きをより興味深いものにしている。**

(<small>[GlennLuk, 2025-01-30投稿](https://x.com/GlennLuk/status/1885695937676259801)</small>)

これに対し、Netscapeブラウザを開発したMarc Andreessen氏がごく短く返信していたことが、また象徴的です。

<blockquote class="twitter-tweet"><p lang="qst" dir="ltr">Yep. <a href="https://t.co/JXUcbKmiL1">https://t.co/JXUcbKmiL1</a></p>&mdash; Marc Andreessen 🇺🇸 (@pmarca) <a href="https://twitter.com/pmarca/status/1885696532110041545?ref_src=twsrc%5Etfw">February 1, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

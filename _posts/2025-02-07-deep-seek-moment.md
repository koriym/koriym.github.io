---
layout: post
title: "DeepSeek Moment"
date: 2025-02-01 00:00:00 +0900
comments: true
categories: ["blog"]
tags:
- AI
---

# DeepSeek Moment - Lex Fridmanポッドキャスト #459を聞いて

<iframe width="560" height="315" src="https://www.youtube.com/embed/_1f-o0nqpEI?si=EEDkYxiOdlCWEdEy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Lex Fridman氏がホストするポッドキャスト、[DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters | Lex Fridman Podcast #459
](https://youtu.be/_1f-o0nqpEI?si=_N93BXUfVMfaXmKC) における、Dylan Patel氏（Semi Analysis）とNathan Lambert氏（Allen Institute for AI）の対談は約5時間超(!)にわたる大作で、AIの最新動向について類をみない圧倒的なスケールです。咀嚼するため、聞き取りと翻訳を行い、記事にまとめました。

ポッドキャストでは何度も「DeepSeek Moment」と称するように、世界のAI業界が劇的変化を遂げている転換点を描き出す、非常に濃厚な内容でした。DeepSeekの革新、メガクラスターの台頭、米中AI競争電力インフラの限界や半導体覇権の綱引き...。技術と地政学が交差する歴史的瞬間を捉えた対談です。

---

## 目次

1. [イントロダクション — 深まるAI競争と「DeepSeek Moment」](#introduction)
2. [DeepSeekの技術革新 — 制約から生まれたブレークスルー](#deepseek)
3. [インフラ革命 — 電力がAIの新たなボトルネックに](#infrastructure)
4. [地政学との交差 — 技術覇権競争の新局面](#geopolitics)
5. [メガクラスターの時代 — 人類史上最大の計算機構築競争](#megacluster)
6. [トレーニングの革新 — 「Chain of Thought」の衝撃](#chain-of-thought)
7. [インフラのボトルネック — 推論コストの課題](#bottleneck)
8. [オープンソースの未来 — AIモデルはどこまで公開できるのか](#opensource)
9. [未来への示唆 — スピードとリスクをどう捉えるか](#future)

---

<a name="introduction"></a>
## イントロダクション — 深まるAI競争と「DeepSeek Moment」

まず、このエピソード自体がどんな背景を持つのか登場人物をみてみましょう。

### ポッドキャストの趣旨とゲスト

- **Lex Fridman**: MITの研究者であり、「AI」「ロボティクス」「宇宙」「自動運転」など多彩な分野の最先端ゲストと対談を行うポッドキャストを運営。技術と共に人文的、哲学的な視点も交えつつ技術の深い部分を掘り下げるスタイルが特徴です。これまでも多彩なゲストを迎えており、今回のエピソードもその流れを汲むものです。
- 
- **Dylan Patel**: 半導体・ハードウェア分野の分析メディア「Semi Analysis」を主宰し、NVIDIAやTSMC、Intelなどシリコン業界の動向を鋭くウォッチしています。AIのインフラ面にも精通しており、とくにGPU供給やデータセンター構築戦略の最前線情報を持つ。この人の持つ情報の量と質は圧巻です。

- **Nathan Lambert**: Allen Institute for AI（通称AI2）の研究者であり、大規模言語モデル（LLM）の学習手法やポストトレーニング（特に強化学習＋検証可能タスク）などを専門る。論文だけでなく、自身でブログを運営し、業界動向をまとめています。

私自身はAIの専門知識をほとんど持ち合わせていませんが、一ユーザーとしてDeepSeekについてかねてより注目してて去年7月にAPIの利用を開始してました。そのころの料金はChatGPTのものに対して1/100ぐらいでどうしてこんな価格が可能なのだろうかと不思議でした。私自身は彼らの技術のすごさを理解することはできませんが、AIの専門家の人たちが特別な視線で見てるのは気がつきました。彼らの技術や組織について分かってくるにつれて、彼らは中国でも特異な組織であることが分かってきました。同時になぜこんなすごい人たちが全く注目を浴びていないのだろうかと不思議にも思ってました。

なのでLex FridmanがDeepSeekの事について取り上げる予告をしていた時から楽しみにしていました。そして、このエピソードは私の期待をはるかに超えるものでした。

### このポッドキャストの価値

AIや半導体、米中問題などを取り上げる番組はありますが、「最新鋭の技術と地政学やインフラ構築」を一気に横断して話せる人材は貴重です。Dylan氏とNathan氏は、 それぞれ“物理と電力”という観点（ハードウェア側）、“学習手法とモデルアーキテクチャ”という観点（ソフトウェア側） 深く理解しており、さらに世界のAI企業が競争する現場情報もカバーできる希少な組み合わせです。  

R1がリリースされる前はXで"DeepSeek"と検索すると、数は少ないですが識者たちの深い考察が見つかってました。しかし、R1がリリースされた後はゴシップのようなノイズのようなそういう投稿が一気に増えました（特に日本語のもの）。いろんな人がいろんな語り口で投稿していましたが、中でもNathan Lambertさんの投稿がフェアで興味深いとか感じてたので、このポッドキャストに彼が出演すると知って、これは見逃せないと思いました。

今回の対談を通じて、多くの人が「いま本当に起きている最前線のこと」を**一度に俯瞰**できた点に俯瞰できたことに意義があると思います。その中心に「DeepSeek」という中国発の企業が登場し、大きな転換点を生み出している、というのがこのポッドキャストで何回も出てくる「DeepSeek Moment」なのでしょう。DeepSeekは単に中国が「うまくやって」安くて高性能なAIを提供している、というだけの話ではありません。技術的な革新と地政学的な要因が絡み合っている、それが明らかになっていくのがこの対談の面白さです。

---

<a name="deepseek"></a>
## 2. DeepSeekの技術革新 — 制約から生まれたブレークスルー

鍵となるのが、中国系AI企業である**DeepSeek(深度求索)**の躍進です。彼らはもともとの母体がクオンツ系のヘッジファンドで、その事業のためとその資金を背景にGPUを大量保有していたとされ、高度な自然言語処理モデル「DeepSeek-V3」とリーゾニング特化モデル「DeepSeek-R1」をリリースしました。

### 2.1 H800の制限を逆手に取った最適化

通常、AIの最先端モデルを学習・推論するには**NVIDIA H100**やA100といっ高性能GPUが好まれますが、中国向けにはFLOPsや通信帯域が抑えられた“H800”しか正規ルートで入手できません。DeepSeekはそれをめげずというか、逆手に取ったというか

1. **CUDAよりも低いPTXレベルで通信を制御**
2. **NCCL（NVIDIA標準ライブラリ）に頼らない独自通信アルゴリズム**
3. **GPUのSMリソースを通信と演算で動的に割り当てる**

というアプローチを構築して。従来は「通信帯域がボトルネックになるから大規模学習は無理」と言われていた（規制したいた）ところを、ソフトウェア側の超lowレイヤーの最適化でカバーしました。

#### 2.1.1 具体例：SM単位での手動スケジューリング
NVIDIAが提供するNCCLは、GPU間通信をオートマティックにやってくれる便利なライブラリなのだそうですが、H800など制限GPUの帯域幅には最適化されてません。それでDeepSeekはSM（Streaming Multiprocessor）ごとに「いつ演算を実行し、いつ通信を行うか」を精密にスケジューリングし、アイドル時間を最小化してわずかな帯域しかなくてもフルに使えるようし、大規模モデルの効率を飛躍的に向上したそうです。

### 2.2 MoEの大胆なスパース化 — 6000億が実質370億？

さらにDeepSeekは、**Mixture of Experts（MoE）** を極度にスパース化することで「実際に使う計算量」を抑える手法を導入。これにより、理論上は6000億パラメータという超巨大モデルでも、1回の推論時には370億パラメータ程度しか実行しないですむため、計算コストが圧倒的に削減されます。

従来のMoEは8エキスパート中2つを選ぶ…ぐらいの規模だったものを256エキスパート(!)
中8つを選ぶという大幅"スパース化"を行ったのだそうです。こうした極端な設計は、技術的な困難を伴うものですがそれをやりとげてリリースしたのが、専門家の間で驚きをもって迎えられた理由なんだそうです。


### 2.3 R1の「推論可視化」とチェインオブソート公開

もう一つの革新が、**DeepSeek-R1**による「思考過程の可視化」です。ユーザーがチャットに問いかけると、膨大な“自問自答”のテキストが示され、最後に結論（返答）が出されます。ChatGPTと違い、チェイン・オブ・ソートのすべて開示されます。一方、ChatGPTは安全性などの理由で要約の形でCOTを見せて（隠して）います。

モデルの誤りを発見しやすくなる、学習研究者にとっては宝の山のようなデータが得られる一方、安全性や機密性の問題も孕む など利点・課題が同居しています。対談では、この試みが「モデルが推論の透明性を得る大きなトレンド」になる可能性がある、と非常に高く評価されていました。

---

<a name="infrastructure"></a>
## 3. インフラ革命 — 電力がAIの新たなボトルネックに

DeepSeekの話が突出して注目されるのは、「制約を逆手に取った技術ブレークスルー」があまりに鮮烈だからだそうです。しかし本ポッドキャスト全体では、電力・冷却インフラがAI拡張を進める上で最大のボトルネックになる、という文脈も多く論じられました。

### 3.1 電力コストのインパクトが増大

いま、GPUを何万枚も集約するメガクラスターが乱立し始めています。これに伴い、電力消費がかつてないペースで膨張し、「電気が足りない」「発電所の増強が追いつかない」など、前例を見ない課題が顕在化しています。OpenAIやMicrosoftが推し進める**Stargate**は、なんと2.2ギガワット（GW）という莫大な電力を消費する計画とされます。この2.2GWとは、中規模な都市の総使用電力を上回るレベルで、調べると佐賀県の玄海原子力発電所の出力に匹敵するほどの電力のようです。

さらに複数のAI企業が同様の規模でデータセンターを建設すれば、その電力はどこから持ってくるのか？ 送電網の整備は？ 環境負荷は？ など、従来のクラウド時代には想定されなかったスケールの問題が噴出します。

ポッドキャストの中で興味深いエピソードの１つ

> MetaはPyTorchに「PyTorch.powerplant_no_blow_up」というフラグを追加しました。これは、GPUがアイドル状態になると偽の計算を行い、電力使用量の急激な変動を抑えるものです。なぜなら、大規模な電力スパイクが発生すると、発電所そのものに影響を与えてしまうからです。

つまり、もし計算が止まって電力消費が急激に低下したときに、発電所そのものにダメージを与えないようにするために『偽の計算』を行い無駄な電力消費をして"powerplant_no_blow_up"を実現するというものです。信じられないようなフラグ名です！

### 3.2 各社の電力戦略の違い

それぞれの戦略の違いが興味深いです。

- **OpenAI / Microsoft**: 連邦政府の協力（あるいは規制緩和）も得て、大規模発電所の近隣にメガデータセンターを建てる戦略。これにより電力のロスを最小限にできる。既存のエネルギー企業と連携しながら急ピッチで建設中。

- **Meta**: 既存のインフラを拡張しつつ、天然ガスや再生可能エネルギーなどを段階的に導入する。レコメンドや広告システムにAIを使うため、推論需要に応じてインフラ規模を増やす。Googleとも似たような分散戦略をとり、大量の小～中規模データセンターを地域分散。

- **xAI**: Elon Muskの独断的な「突き抜けた行動力」で、メンフィスの古い工場を買収・改装し、独自の液冷システムを導入。発電設備も自前で整備して、電力供給の柔軟性を極限まで高めている。時間短縮と高密度化の両立を目指す。Elonらしい大胆な戦略。

### 3.3 冷却技術の進化

既存データセンターの多くは空冷を前提としていますが、GPUクラスタは発熱量がCPUクラスタよりはるかに多いため、冷却に限界が生じます。そこで**液冷**を積極採用するケースが増え、xAIなどは「全面液冷」への挑戦を早くも進めています。  

液冷の利点としては、
- 密集度を高められ、同じスペースでより多くのGPUを動かせる
- オーバーヒートや排熱トラブルが大幅に減る
- 配管設計の難易度は高いが、運用が軌道に乗ると安定動作しやすい

というメリットが挙げられます。反面、初期投資やエンジニアリング負荷が高く、障害が起きた際のリカバリーが複雑といったデメリットも。メガクラスターにおいては、これらを総合的に計算しながら企業が個別の設計を模索しているのが現状のようです。

---

<a name="geopolitics"></a>
## 4. 地政学との交差 — 技術覇権競争の新局面

### 4.1 米中のAI競争と輸出規制

番組の大きな山場の一つが、米国の輸出規制と、中国がそれに対処する中で発揮する「制約下でのイノベーション」でした。DeepSeekの事例は、まさに最新GPUが買えずともPTXレベル最適化でパフォーマンスを得られることを証明し、米中の“いたちごっこ”のような構図を映し出しています。  

一時的に中国のGPU供給は厳しくなっているものの、**密輸や第三国経由の調達**、さらに自社開発のASICやNPUで代替する動きまであり、完全な封じ込めは難しいとも指摘されました。Dylan氏いわく、ByteDance（TikTokの親会社）がOracleやGoogleのクラウドから大量のGPUをレンタルしているという話もあり、輸出規制をすり抜けるルートは意外と多いようです。

またポッドキャストでは触れられませんでしたが、『ファーウェイの Ascend 910Bチップに自社モデルを移植して実行した。動的精度調整技術によって、同じタスクで5％しか性能が落ちなかったが、コストは70％下がった。』という投稿もあり、これがNvidiaがクラッシュした原因だという話もあります。

https://x.com/olalatech1/status/1883983102953021487

### 4.2 中国政府の1兆人民元投資とその意図

DeepSeekの成功を受け、中国政府が約1600億ドル規模のAI投資を発表したことは大きく報じられました。これがHuaweiやAlibaba、Baiduなどの他の中国企業にも波及し、さらなるGPU需要が急増する可能性もあります。

同時に、台湾TSMCの存在がリスク要因として浮上します。もし地政学的に不安定化すれば、世界の先端半導体生産が大打撃を受け、AIハードウェア開発が一挙に停滞する可能性があります。そこで米国はTSMCを国内生産に引き込もうとCHIPS法などを駆使していますが、短期間でファウンドリーを作れるわけでもなく、先の見えない駆け引きが続いている状況みたいです。

### 4.3 ソフトパワーとしてのAI

AI技術が軍事・経済だけでなく「国家ブランディング」や「情報拡散」に直結することも興味深い論点として挙がりました。巨大言語モデルが自然言語で説得的な文章を生成したり、SNSの推薦アルゴリズムを支配したりすると、世論形成や政治介入の手段に使われかねません。  
この点についても、米中の攻防は激化が予想されますし、EUなどが規制策を検討しているがスピードが追いつかない可能性も示唆されました。

---

<a name="megacluster"></a>
## 5. メガクラスターの時代 — 人類史上最大の計算機構築競争

### 5.1 クラスタ規模が桁違いに
2020年前後、GPT-3が数千台～1万台のGPUを使って数週間学習したというだけでも大ニュースだったのが、今は**10万枚～20万枚規模のGPUクラスター**が複数企業によって同時多発的に構築されています。番組では、Elon MuskのxAIがメンフィスで20万台まとめ、OpenAIがStargateで10万台規模（将来的にはさらに倍かそれ以上）を見込むといった数字がポンポン飛び出していました。  

こうなるともはや「データセンター」というより「小さな発電所の横に巨大工業プラントを建てる」イメージに近く、エネルギー・冷却・ネットワークすべてが前例のないスケールに到達します。人類史における最大級のコンピュータ構築レースと言えるでしょう。

この背景に各社が「AGIの実現が数年以内」と見ていて、それを実現するためのあらゆる施作を急いでいるという点がありそうです。

### 5.2 集約型vs分散型

そのクラスターの構築方法にも違いがあります。

- **集約型（例: xAI, 一部のStargateプラン）**
    - 全GPUを1ヶ所に集めることで、モデル学習時の通信レイテンシを最小化し、大規模並列学習を効率化。
    - ただし一度に数ギガワット単位の電力が必要となり、土地や送電網の許認可が大変。故障リスクや自然災害リスクも高い。
- **分散型（例: Meta, Google）**
    - 全世界や国内各地にデータセンターを置き、ユーザーに近い場所で推論を行う仕組み。
    - スケールは大きいが、学習時には複数拠点をネットワークで繋ぐ必要があり、ブロックしない通信と同期が課題に。
    - 災害リスクや政治リスクは分散できるが、アーキテクチャ設計が一層複雑になる。

### 5.3 ソフトウェアスタックと低レイヤー最適化の重要性

DeepSeekが証明したように、超大規模クラスタを動かすには単にGPUを並べるだけでなく、**通信ライブラリやタスクスケジューリング、メモリ管理**など、ソフトウェアスタック全体を最適化する必要があります。すでに紹介したMetaの「電力スパイクを抑制するために余計な演算を入れて発電所を安定稼働させる」話など、従来では考えられなかったトリッキーな対応も行われています。

---

<a name="chain-of-thought"></a>
## 6. トレーニングの革新 — 「Chain of Thought」の衝撃

### 6.1 DeepSeek-R1が導く新しい対話体験

DeepSeek-R1のように、推論ステップを大量に可視化するスタイルは、「ユーザーがモデルの脳内をリアルタイムで眺めている」という点がユニークです。チェインオブソート（思考の連鎖）はこれまで秘匿されがちでしたが、すべてを見せると、かえってユーザーが途中の誤りに気づきやすくなるなどのメリットが大きいと。

しかし、他社モデルがあまりやらないのは、「推論の中間出力が誤情報を含んだり、ユーザーが混乱したりする可能性がある」という懸念があるためとも言及していました。OpenAIのGPT-4などは「チェインオブソートが毒素や不適切情報を生む恐れがある」として非公開を標準としています。

### 6.2 強化学習×検証可能タスクの融合

R1の事例からもう一つ浮かび上がるのが、「強化学習（RL）とチェインオブソートの親和性」。モデルが逐次的に解法を試し、検証可能なゴールが得られるタスク——たとえば数学の正解を出す、コードをコンパイルしてテストをパスするといった『閉じた世界』のケースでは、チェインオブソートのステップを評価しながら学習を進められます。ちなみにDeepSeekが力をいれているのは「数学」「コーディング」「自然言語」の３つの分野です。（政治や歴史ではありません）

Andrej Karpathy氏がAlphaGoでの成功になぞらえて言うように、**「模倣学習」から「試行錯誤と自己プレイ」に移行したとき、突然モデルが人間の直感を超える解を得る**状況が発生しやすい。これがAI技術の次の大きな段階かもしれないことを強調していました。

> 私が好きな単純な例はAlphaGoで、学習の方法は２つあり、1）熟練棋士の真似をして学習している、2）ゲームに勝つために強化学習する。（勝ち負けを報酬として繰り返し自分同士で対戦をする）
> ディープラーニングのほとんどすべての衝撃的な結果、そしてすべての*マジック*の源は常に2である。2こそがあなたを驚かせる。
> — Andrej Karpathy氏

https://x.com/karpathy/status/1883941452738355376

この投稿は多くRTされ、反響を呼んでいたのでよく覚えてましたがポッドキャストでもとりあげられました。o1やR1などreasoning modelが注目される証です。またいくつかの発言がこれが"1"(o1, R1)に過ぎないこと、つまりまだはじまりに過ぎないということを示唆していました。

### 6.3 倫理的・社会的インパクト

モデルの思考過程をすべて公開することが標準化すれば、利用者がモデルの誤りをすぐ訂正できるという利点はあるものの、同時に「中間ステップで差別的表現や有害発言が生成された場合、どう対処するか」「プライバシーやセキュリティ情報が誤って流出しないか」などの課題も大きくなりますがそこの基準は欧米と中国で違うと考えるのが自然です。安全性を保ちながらチェインオブソートを可視化する方法は、これから研究が進む領域でしょうが、AGIに"lazer focus"を当てるためにはどうなのでしょうか。

---

<a name="bottleneck"></a>
## 7. インフラのボトルネック — 推論コストの課題

### 7.1 トレーニングと推論のコスト構造

ディープラーニングモデルにおけるコストは大きく分けて「学習（トレーニング）」と「推論（インファレンス）」に分かれます。以前は「巨大モデルを学習するコストが圧倒的」と言われていましたが、モデルが実社会に大量導入されるフェーズに入ると、**日々の推論量**こそが大きな負担になる可能性が指摘されています。  

DeepSeekは効率的な学習を実現した半面、R1を本格運用する際の推論リソースが追いつかず、新規ユーザー停止に至ったエピソードが示されます。OpenAIのChatGPTも大量に利用されるうちにGPUリソース不足が懸念され、料金プランが複数用意されるなど、エンドユーザー数と運用コストの兼ね合いがシビアになっています。

私はAnthropicやOpenAIのような企業が無料、もしくは$20/月という定額の料金でサービスされてる時になんと民主的なのだろうか、だれもがAIを使えるのは素晴らしいと思いましたけどこれからは、それはなかなか難しいのかもしれないとも感じました。

### 7.2 リースニング（推論）モデルと高額APIの出現

OpenAIが発表した**o1**や**o3-mini**など「リースニング特化型」のモデルが登場し、驚くほど優秀な思考過程を持つ反面、推論時に内部的に数多くの試行を行うため、1クエリあたりのGPU時間が跳ね上がるケースがあります。価格が100万トークンあたり数十ドルに達することもあり、企業が大規模に導入するには抵抗感があるレベルです。  
DeepSeek-R1のチェインオブソート公開も同様の性質を持ち、1回の対話で膨大な中間トークンを生成する分、運用コストが増すというジレンマが指摘されます。

### 7.3 今後の解決策や見通し

- **専用ハードウェア**: NVIDIAの新アーキテクチャ（たとえばBlackwell GPU）や、Googleの次世代TPU、MetaやAmazonの自社開発チップなど、推論特化の設計が進むことでコストを下げられる可能性があるそうです。

- **モデル圧縮・蒸留**: LLMを小型モデルへ蒸留し、推論を軽量化する手法が盛んに研究されています。DeepSeekも部分的にOpenAIモデルを蒸留したかもしれない、という疑惑が報道されました。しかし番組の中ではOpenAPIが出力したものとその生成でつくられたコンテンツに区別がつかないし、そもそも違反したとしてもアカウント停止ぐらいの処置しか取れようが無い。なんでそんな事を話題にするんだという論調でした。

- **分散推論**: 大規模サービスでは、近場のサーバーで低レイテンシに推論を行う。

---

<a name="opensource"></a>
## 8. オープンソースの未来 — AIモデルはどこまで公開できるのか

### 8.1 DeepSeekのMITライセンスインパクト

DeepSeekがR1モデルを比較的自由度の高いライセンス（クローズドデータのオープンモデル）で公開したのは、業界に衝撃を与えました。過去にはMetaのLlamaが一部オープンウェイトを出して話題になりましたが、軍事利用等に制限があるライセンス形態で、本当の意味での「オープンソース」とは言いづらい面があった。  

対してDeepSeekは商用利用も含め緩やかなライセンスを示し、「中国企業の成果が一気に世界へ波及する」かも？と状況を生み出しています。ただ、本当にこのモデルを大規模に再訓練・改良しようとすれば、数万～数十万台のGPUが必要かもしれず、実際には大手企業や国レベルのプレイヤーに限られるのも事実です。

### 8.2 オープンソースAIの本質的障壁

> (00:06:17)
オープンソースAIの定義や本質は、まだ議論の真っ最中です。オープンソースソフトウェアの世界では、改変の自由や自分で引き継ぐ自由など、多くの自由を認める歴史がありますが、それをAIに当てはめるとどうなるかは、まだ固まっていない面があるんですね。

> Lex Fridman
(00:07:45)
オープンソースの世界を「完全にコードもデータもウェイトも公開」が極端な一端とすると、DeepSeekはそれほどオープンではないにせよ、少なくとも「オープンウェイト」でMITライセンスに近い形。つまりオープンソースムーブメント的には「善玉」に近い感じなんでしょう？

>Nathan Lambert
(00:08:13)
ええ。DeepSeekはAIの知見を広めるために素晴らしい貢献をしていると思います。彼らの論文は非常に詳細で、世界の他の研究チームがトレーニング手法を改善するのに役立つ情報が多い。ライセンス面で言うと、DeepSeek-R1モデルは非常に寛容なライセンス、つまりMITライセンスです。これは商用利用において制限がなく、ユースケース制限もなく、生成された出力を使って合成データを作るのも自由ということです。

ソフトウェアの場合、コードを公開すれば誰もがビルドして利用可能です。しかし、AIモデルでは
1. 
2. **膨大な学習データ**（著作権やプライバシー問題）
2. **学習自体が莫大なコスト**
3. **推論運用にも大規模なインフラが要る**  
   という三大要求というか、三重苦があり、「公開すれば終わり」というわけにはいきません。それでもコミュニティ主導で小型モデルを蒸留したり、微調整したりする動きは活発で、オープンソースのメリット（透明性や改変自由度）はあります。

   この辺りは「オープンなアルゴリズム」と「巨大クローズドインフラ」のせめぎ合いという複雑な図式になり、今後は国際的な規制や著作権法の見直しが焦点となってくるかもしれません。何をオープンソースモデル

### 8.3 別種のエコシステム形成


さらに、完全オープンにこだわらずとも、「コラボレーティブな形での“準オープン”モデル」が台頭する可能性も。具体的には、学習コードと最終ウェイトの一部を公開し、商用利用には定額ライセンスを課すなど、両立策がありえます。DeepSeekが次にどのような戦略を取るのか、OpenAIやGoogleがオープン化の度合いをどう変えていくのか、今はまだ読み切れません。

---

<a name="future"></a>
## 9. 未来への示唆 — スピードとリスクをどう捉えるか

### 9.1 かつてない革新ペース

総合すると、今回のポッドキャストは「AI産業がどれほど急速に加速しているか」を生々しく描写しています。1年単位でクラスタ規模が1桁、2桁と増えていくような状況は、ムーアの法則をはるかに上回るペースとも言われます。  
AlphaGoが囲碁で勝利した2016年頃から、GPT-3が衝撃を与えた2020年を経て、いま2025年には「思考過程を完全公開する超大規模モデル」が次々と登場している。この速度であと3年、5年進めば、AGIの一部形態が先に到来する、と考えるコンセンサスがあるかのような雰囲気がありました。

### 9.2 リスク管理と国際協調

これだけのスピード感でインフラや技術が拡大すると、負の側面のコントロールが追いつかない可能性が高いのは多くの専門家が懸念しています。

- **安全保障**: 高性能AIが軍事や工作活動に使われる懸念。米中だけでなく他国や非国家組織が参入するリスクも。
- **倫理・公平性**: AIが生む差別的・偏った結果をどう是正するか、チェインオブソートを含めたコンテンツリスクをどう扱うか。
- **環境負荷**: 数ギガワット単位で電力を消費するデータセンターが乱立することで、エネルギー確保やCO2排出は大丈夫か。

AIは人類の課題解決に多大な可能性を持ちますが、問題が山積みでもあります。この「制御不能に見える加速」を社会がどう扱うかは、まだ誰も経験したことがない領域であり、ローカルなルールではなく世界的な合意や協調が不可欠だという意見も出始めました。

### 9.3 人類の知的能力を広げるか、それとも破壊するか

対談の中では、AIが人間の思考を補強する素晴らしい道具になる未来が語られつつ、「同時に非常に危険な側面も大きい」との認識が共有されていました。DeepSeek-R1のような思考過程可視化システムは、使い方によっては革命的に生産性を上げ、学習・研究ツールとしても非常に価値が高い。しかし、それが同時に悪用され、まったく別の目的（社会操作、セキュリティ侵害、自動化された誤情報拡散など）に使われるリスクも拡大します。こうした善と悪の二面性をどうバランスさせるかこそ、いま問われているわけです。

---

## 終わりに

今回のエピソードに登場した「DeepSeek Moment」は、AI業界がまさに歴史的転換点を迎えていることを象徴しています。

1. 中国企業が輸出規制下でH800の帯域制限を逆手に技術革新
2. アメリカ企業がStargateなどでギガワット級のメガデータセンターを爆速で建設
3. オープンソースかクローズドか、巨大モデルをどう扱うかの議論が熱を帯びる
4. 米中間の輸出規制や地政学的な緊張がテック業界をさらに加速
5. リースニングモデルの推論コスト高騰や、チェインオブソートの可視化がもたらす新課題

これらが一挙に押し寄せている状況は、今後数年でさらにカオスと化すことが想定されます。Lex Fridman氏のポッドキャスト#459は、その「カオスの萌芽」を専門家2名の知見を通して克明に描いており、AIの未来に興味を持つ人は必見・必聴の内容と言えます。

どのようなシナリオであれ、AIは今後もスケールアップを続け、社会と経済を大きく揺さぶる存在になるでしょう。そのスピード感は驚異的で、かつての産業革命とは比べものにならないほど急激です。  

この激流の中で、技術者・企業・政府・市民社会がどのような選択を重ねていくのか。ビックテックの中に突如でてきたかに見える、DeepSeek R1のリリースはこれまで顕在化していた技術と地政学がどのように交差するのかに新しい光を当てたと言えるでしょう。

このポッドキャストでもこの「DeepSeek Moment」は今後語り継がれるものになるだろうと伝えていました。

私が番組前から特に気になっていたXの投稿がこれです。

> DeepSeek v3とR1のオープンソースリリースは、1994年にリリースされたMosaicのNetscapeブラウザと似た雰囲気を持っている。
> それはまったく新しい時代の到来を告げるものであり、インターネットの発展における極めて重要な瞬間であった。
> それが中国企業であることが、筋書きをより興味深いものにしている。

https://x.com/GlennLuk/status/1885695937676259801

これに対して、Marc Adreessen氏(Netscapeブラウザの開発者）が一言で答えていました。

<blockquote class="twitter-tweet"><p lang="qst" dir="ltr">Yep. <a href="https://t.co/JXUcbKmiL1">https://t.co/JXUcbKmiL1</a></p>&mdash; Marc Andreessen 🇺🇸 (@pmarca) <a href="https://twitter.com/pmarca/status/1885696532110041545?ref_src=twsrc%5Etfw">February 1, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
